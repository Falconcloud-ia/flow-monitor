#!/usr/bin/env python3
"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë             ‚ò¢Ô∏è  FLOW-MONITOR EXTREME LOAD TEST (NUCLEAR MODE) ‚ò¢Ô∏è             ‚ïë
‚ïë                 Objetivo: > 2,000,000 Peticiones / Ciclo                     ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

Este script utiliza MULTIPROCESSING para evadir el GIL de Python y saturar
completamente la interfaz de red local y el backend.

ADVERTENCIA:
    - Esto consumir√° mucha CPU.
    - Puede causar denegaci√≥n de servicio (DoS) local.
    - Dise√±ado para evaluar infraestructura cr√≠tica.

Uso:
    python load_extreme_test.py --total 2000000 --processes 8 --threads 50
"""

import sys
import os
import time
import multiprocessing
import threading
import requests
import random
from datetime import datetime
from queue import Empty
import argparse

# Agregar path para imports
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
# Importamos clases base pero las re-implementaremos ligeras para velocidad m√°xima
# from intelligence_core import IntelligenceService 
# NOTA: Para velocidad extrema, generaremos payloads "pre-calculados" o ligeros
# invocar toda la l√≥gica de IntelligenceService en el generador de carga es un cuello de botella.
# Simularemos la salida del sensor directamente.

def generate_fast_payload(sensor_id):
    """Genera payload optimizado para velocidad."""
    temp = random.uniform(20.0, 110.0)
    
    # L√≥gica simplificada de "intelligence" para no gastar CPU en el generador
    risk = "LOW"
    if temp > 90: risk = "CRITICAL"
    elif temp > 80: risk = "HIGH"
    elif temp > 60: risk = "MEDIUM"
    
    return {
        "data_original": {
            "sensor_id": sensor_id,
            "timestamp": datetime.now().isoformat(),
            "value": round(temp, 2),
            "unit": "Celsius",
            "_meta": {"status": "STRESS_TEST"}
        },
        "risk_level": risk,
        "prediction_alert": {
            "failure_probability": random.random(),
            "predicted_failure_time": None
        },
        "processed_at": datetime.now().isoformat()
    }

def worker_process(proc_id, target_url, limit_requests, num_threads, shared_counter, error_counter, start_event):
    """
    Funci√≥n que ejecuta un PROCESO completo.
    Lanza m√∫ltiples hilos para realizar peticiones HTTP.
    """
    
    # Esperar se√±al de inicio para coordinar ataque simult√°neo
    start_event.wait()
    
    session = requests.Session()
    # Optimizaci√≥n de conexi√≥n
    adapter = requests.adapters.HTTPAdapter(pool_connections=num_threads, pool_maxsize=num_threads)
    session.mount('http://', adapter)
    
    local_count = 0
    local_errors = 0
    
    def thread_task():
        nonlocal local_count, local_errors
        while True:
            # Verificaci√≥n "suelta" para rendimiento (no lockear en cada iteraci√≥n)
            if shared_counter.value >= limit_requests:
                break
                
            try:
                # Generar payload r√°pido
                payload = generate_fast_payload(f"PROC_{proc_id}_THREAD_{threading.get_ident()}")
                
                resp = session.post(target_url, json=payload, timeout=5)
                
                if resp.status_code == 200:
                    local_count += 1
                    # Actualizar contador compartido en lotes para evitar contenci√≥n de lock
                    if local_count % 100 == 0:
                        with shared_counter.get_lock():
                            shared_counter.value += 100
                else:
                    local_errors += 1
                    with error_counter.get_lock():
                        error_counter.value += 1
                        
            except Exception:
                local_errors += 1
                with error_counter.get_lock():
                    error_counter.value += 1
    
    threads = []
    for _ in range(num_threads):
        t = threading.Thread(target=thread_task)
        t.daemon = True
        t.start()
        threads.append(t)
        
    for t in threads:
        t.join()

    # Actualizar remanente
    rem = local_count % 100
    if rem > 0:
        with shared_counter.get_lock():
            shared_counter.value += rem


def monitor(shared_counter, error_counter, total_target, start_time):
    """Hilo de monitoreo en el proceso principal."""
    try:
        while True:
            time.sleep(1)
            current = shared_counter.value
            errors = error_counter.value
            elapsed = time.time() - start_time
            
            if elapsed == 0: continue
            
            rps = current / elapsed
            progress = (current / total_target) * 100
            
            # Barra de progreso visual
            bar_len = 30
            filled_len = int(bar_len * current // total_target)
            bar = '‚ñà' * filled_len + '‚ñë' * (bar_len - filled_len)
            
            print(f"\rüöÄ [{bar}] {progress:5.1f}% | "
                  f"Req: {current:,} / {total_target:,} | "
                  f"Errors: {errors:,} | "
                  f"RPS: {rps:,.0f}", end="", flush=True)
            
            if current >= total_target:
                break
    except KeyboardInterrupt:
        pass

def main():
    parser = argparse.ArgumentParser(description="Nuclear Stress Test")
    parser.add_argument("--total", type=int, default=2000000, help="Total de peticiones objetivo")
    parser.add_argument("--processes", type=int, default=multiprocessing.cpu_count(), help="N√∫mero de procesos CPU")
    parser.add_argument("--threads", type=int, default=50, help="Hilos por proceso")
    parser.add_argument("--url", default="http://localhost:8001/api/dashboard/process", help="Endpoint destino")
    
    args = parser.parse_args()
    
    print(f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë             ‚ò¢Ô∏è  INICIANDO PRUEBA DE ESTR√âS EXTREMA  ‚ò¢Ô∏è                       ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë üéØ Objetivo:   {args.total:,} peticiones                                     ‚ïë
‚ïë üîß Workers:    {args.processes} Procesos x {args.threads} Hilos = {args.processes * args.threads} Concurrencia ‚ïë
‚ïë üîó Endpoint:   {args.url}                                                    ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    # Objetos compartidos
    manager = multiprocessing.Manager()
    shared_counter = multiprocessing.Value('i', 0)
    error_counter = multiprocessing.Value('i', 0)
    start_event = multiprocessing.Event()
    
    # Crear procesos
    processes = []
    
    # Dividimos el objetivo "virtualmente", pero todos suman al shared_counter
    # Pasamos limit_requests muy alto al worker para que paren por el contador global logic (o aproximado)
    # Mejor: que cada worker pare cuando vea el shared_counter lleno.
    
    print("üî• Preparando ojivas (creando procesos)...")
    
    for i in range(args.processes):
        p = multiprocessing.Process(
            target=worker_process,
            args=(i, args.url, args.total, args.threads, shared_counter, error_counter, start_event)
        )
        processes.append(p)
        p.start()
        
    print("üî• Procesos listos. ¬°LANZANDO ATAQUE!")
    start_time = time.time()
    start_event.set() # Iniciar todos a la vez
    
    # Monitoreo
    monitor(shared_counter, error_counter, args.total, start_time)
    
    # Esperar terminaci√≥n
    for p in processes:
        p.join()
        
    end_time = time.time()
    duration = end_time - start_time
    total_reqs = shared_counter.value
    actual_rps = total_reqs / duration if duration > 0 else 0
    
    print("\n\n" + "‚ïê" * 70)
    print("üìä REPORTE DE INFRAESTRUCTURA - RESULTADO FINAL")
    print("‚ïê" * 70)
    print(f"‚è±Ô∏è  Duraci√≥n Total:      {duration:.2f} segundos")
    print(f"üì® Peticiones Totales:  {total_reqs:,}")
    print(f"‚ùå Errores:             {error_counter.value:,} ({(error_counter.value/total_reqs*100):.2f}%)")
    print(f"‚ö° Throughput (RPS):    {actual_rps:,.2f} req/s")
    print("‚ïê" * 70)
    
    # Evaluaci√≥n para Kubernetes
    print("\nüí° EVALUACI√ìN PARA KUBERNETES & IAC:")
    if actual_rps > 5000:
        print("‚úÖ EXCELENTE RENDIMIENTO: La aplicaci√≥n maneja alta concurrencia.")
        print("   Recomendaci√≥n: Cluster K8s est√°ndar con HPA (Horizontal Pod Autoscaler) basado en CPU.")
    elif actual_rps > 1000:
        print("‚ö†Ô∏è RENDIMIENTO MODERADO: Puede requerir optimizaci√≥n de c√≥digo o m√°s r√©plicas.")
        print("   Recomendaci√≥n: K8s con m√∫ltiples r√©plicas (min 3-5) y caching (Redis).")
    else:
        print("üî¥ CUELLO DE BOTELLA DETECTADO: El backend actual no soporta carga industrial masiva.")
        print("   Recomendaci√≥n: REFACTORIZAR a arquitectura as√≠ncrona pura o Go/Rust para Ingestion.")
        print("   K8s requerir√° escalado agresivo de pods.")

if __name__ == "__main__":
    main()
